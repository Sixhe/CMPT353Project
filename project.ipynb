{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6250685",
   "metadata": {},
   "source": [
    "Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca79d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# May takes few minutes\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5f0e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vancouver polygons: 24 | CRS: EPSG:4326\n",
      "Victoria polygons: 29 | CRS: EPSG:4326\n",
      "Saved QC plot to: reports\\figures\\boundaries_quickcheck_step4.png\n"
     ]
    }
   ],
   "source": [
    "# Step 4.1 — Setup paths, load polygons, ensure CRS, and quick QC plot\n",
    "\n",
    "data_dir = Path('original_data')\n",
    "reports_dir = Path('reports')\n",
    "fig_dir = reports_dir / 'figures'\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "van_geo_path = data_dir / 'van_neighbourhoods.geojson'\n",
    "vic_geo_path = data_dir / 'vic_neighbourhoods.geojson'\n",
    "\n",
    "# Helpers\n",
    "EPSG_WGS84 = 4326\n",
    "\n",
    "def ensure_wgs84(gdf):\n",
    "    if gdf.crs is None:\n",
    "        return gdf.set_crs(EPSG_WGS84)\n",
    "    try:\n",
    "        epsg = gdf.crs.to_epsg()\n",
    "    except Exception:\n",
    "        epsg = None\n",
    "    return gdf if epsg == EPSG_WGS84 else gdf.to_crs(EPSG_WGS84)\n",
    "\n",
    "# Load geospatial data\n",
    "van = gpd.read_file(van_geo_path)\n",
    "van = ensure_wgs84(van)\n",
    "\n",
    "vic = gpd.read_file(vic_geo_path)\n",
    "vic = ensure_wgs84(vic)\n",
    "\n",
    "print('Vancouver polygons:', len(van), '| CRS:', van.crs)\n",
    "print('Victoria polygons:', len(vic), '| CRS:', vic.crs)\n",
    "\n",
    "# Quick QC boundary plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "van.boundary.plot(ax=ax[0], color='steelblue', linewidth=0.8)\n",
    "ax[0].set_title('Vancouver neighbourhoods')\n",
    "ax[0].axis('off')\n",
    "\n",
    "vic.boundary.plot(ax=ax[1], color='darkorange', linewidth=0.8)\n",
    "ax[1].set_title('Victoria neighbourhoods')\n",
    "ax[1].axis('off')\n",
    "\n",
    "out_path = fig_dir / 'boundaries_quickcheck_step4.png'\n",
    "fig.tight_layout()\n",
    "fig.savefig(out_path, dpi=150)\n",
    "plt.close(fig)\n",
    "print(f'Saved QC plot to: {out_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "152e9114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded van_listings_April2025.csv.gz: shape=(5460, 79)\n",
      "Loaded van_listings_June2025.csv.gz: shape=(5090, 79)\n",
      "Loaded vic_listings_March2025.csv.gz: shape=(3712, 79)\n",
      "Loaded vic_listings_May2025.csv.gz: shape=(3478, 79)\n"
     ]
    }
   ],
   "source": [
    "# Step 4.2 — Load listings (.csv.gz) and inspect\n",
    "\n",
    "data_dir = Path('original_data')\n",
    "\n",
    "# Auto-discover input files that look like listings exports for 2025\n",
    "listing_paths = sorted(data_dir.glob('*listings*2025*.csv.gz'))\n",
    "listing_files = [p.name for p in listing_paths]\n",
    "if not listing_files:\n",
    "    # Fallback to the known four if glob finds nothing\n",
    "    listing_files = [\n",
    "        'van_listings_April2025.csv.gz',\n",
    "        'van_listings_June2025.csv.gz',\n",
    "        'vic_listings_March2025.csv.gz',\n",
    "        'vic_listings_May2025.csv.gz'\n",
    "    ]\n",
    "    listing_paths = [data_dir / f for f in listing_files]\n",
    "\n",
    "loaded = {}\n",
    "for fpath in listing_paths:\n",
    "    df_l = pd.read_csv(fpath, compression='gzip', low_memory=False)\n",
    "    loaded[fpath.name] = df_l\n",
    "    print(f\"Loaded {fpath.name}: shape={df_l.shape}\")\n",
    "\n",
    "# schema candidates\n",
    "candidates = {\n",
    "    # core fields used in Step 4.3\n",
    "    'price': ['price'],\n",
    "    'neighbourhood': ['neighbourhood_cleansed', 'neighbourhood'],\n",
    "    'date': ['last_scraped', 'calendar_last_scraped'],\n",
    "    'latitude': ['latitude'],\n",
    "    'longitude': ['longitude'],\n",
    "    'estimated_revenue_l365d': ['estimated_revenue_l365d'],\n",
    "    'estimated_occupancy_l365d': ['estimated_occupancy_l365d'],\n",
    "    'availability_365': ['availability_365'],\n",
    "    'availability_30': ['availability_30'],\n",
    "    'reviews_per_month': ['reviews_per_month'],\n",
    "    'number_of_reviews': ['number_of_reviews'],\n",
    "    'amenities': ['amenities'],\n",
    "    'property_type': ['property_type'],\n",
    "    'room_type': ['room_type'],\n",
    "    'accommodates': ['accommodates'],\n",
    "    'bathrooms': ['bathrooms'],\n",
    "    'bathrooms_text': ['bathrooms_text'],\n",
    "    'bedrooms': ['bedrooms'],\n",
    "    'beds': ['beds'],\n",
    "    'host_is_superhost': ['host_is_superhost'],\n",
    "    'host_since': ['host_since'],\n",
    "    'license': ['license'],\n",
    "    'calculated_host_listings_count': ['calculated_host_listings_count'],\n",
    "    'minimum_nights': ['minimum_nights'],\n",
    "    'maximum_nights': ['maximum_nights'],\n",
    "}\n",
    "\n",
    "# returns the original column name\n",
    "def pick_exact(df, options):\n",
    "    lower_to_orig = {c.lower(): c for c in df.columns}\n",
    "    for opt in options:\n",
    "        c = lower_to_orig.get(opt.lower())\n",
    "        if c:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "schema_map = {}\n",
    "for fname, df_l in loaded.items():\n",
    "    mapping = {k: pick_exact(df_l, v) for k, v in candidates.items()}\n",
    "    schema_map[fname] = mapping\n",
    "\n",
    "# schema_map # check schema mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eee80c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified listings shape: (17740, 28)\n",
      "city       month  \n",
      "Vancouver  2025-04    5460\n",
      "           2025-06    5090\n",
      "Victoria   2025-03    3712\n",
      "           2025-05    3478\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Columns: ['listing_id', 'city', 'month', 'neighbourhood', 'price', 'last_scraped', 'latitude', 'longitude', 'estimated_revenue_l365d', 'estimated_occupancy_l365d', 'availability_365', 'availability_30', 'reviews_per_month', 'number_of_reviews', 'amenities', 'property_type', 'room_type', 'accommodates', 'bathrooms', 'bathrooms_text', 'bedrooms', 'beds', 'host_is_superhost', 'host_since', 'license', 'calculated_host_listings_count', 'minimum_nights', 'maximum_nights']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>city</th>\n",
       "      <th>month</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>price</th>\n",
       "      <th>last_scraped</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>estimated_revenue_l365d</th>\n",
       "      <th>estimated_occupancy_l365d</th>\n",
       "      <th>...</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bathrooms_text</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_since</th>\n",
       "      <th>license</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>maximum_nights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13188</td>\n",
       "      <td>Vancouver</td>\n",
       "      <td>2025-04</td>\n",
       "      <td>Riley Park</td>\n",
       "      <td>136.0</td>\n",
       "      <td>2025-04-10</td>\n",
       "      <td>49.247730</td>\n",
       "      <td>-123.105090</td>\n",
       "      <td>34680.0</td>\n",
       "      <td>255</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1 bath</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>t</td>\n",
       "      <td>2009-11-04</td>\n",
       "      <td>25-156058</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13358</td>\n",
       "      <td>Vancouver</td>\n",
       "      <td>2025-04</td>\n",
       "      <td>Downtown</td>\n",
       "      <td>198.0</td>\n",
       "      <td>2025-04-10</td>\n",
       "      <td>49.281174</td>\n",
       "      <td>-123.125931</td>\n",
       "      <td>50490.0</td>\n",
       "      <td>255</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1 bath</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t</td>\n",
       "      <td>2009-11-07</td>\n",
       "      <td>25-157257</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16254</td>\n",
       "      <td>Vancouver</td>\n",
       "      <td>2025-04</td>\n",
       "      <td>Hastings-Sunrise</td>\n",
       "      <td>680.0</td>\n",
       "      <td>2025-04-10</td>\n",
       "      <td>49.277210</td>\n",
       "      <td>-123.040860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1 bath</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f</td>\n",
       "      <td>2009-12-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16611</td>\n",
       "      <td>Vancouver</td>\n",
       "      <td>2025-04</td>\n",
       "      <td>Grandview-Woodland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-04-10</td>\n",
       "      <td>49.263390</td>\n",
       "      <td>-123.071450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 bath</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>2009-11-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>365</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17765</td>\n",
       "      <td>Vancouver</td>\n",
       "      <td>2025-04</td>\n",
       "      <td>Mount Pleasant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-04-10</td>\n",
       "      <td>49.261320</td>\n",
       "      <td>-123.108450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 bath</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>21-156705</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id       city    month       neighbourhood  price last_scraped  \\\n",
       "0       13188  Vancouver  2025-04          Riley Park  136.0   2025-04-10   \n",
       "1       13358  Vancouver  2025-04            Downtown  198.0   2025-04-10   \n",
       "2       16254  Vancouver  2025-04    Hastings-Sunrise  680.0   2025-04-10   \n",
       "3       16611  Vancouver  2025-04  Grandview-Woodland    NaN   2025-04-10   \n",
       "4       17765  Vancouver  2025-04      Mount Pleasant    NaN   2025-04-10   \n",
       "\n",
       "    latitude   longitude  estimated_revenue_l365d  estimated_occupancy_l365d  \\\n",
       "0  49.247730 -123.105090                  34680.0                        255   \n",
       "1  49.281174 -123.125931                  50490.0                        255   \n",
       "2  49.277210 -123.040860                      0.0                          0   \n",
       "3  49.263390 -123.071450                      NaN                          0   \n",
       "4  49.261320 -123.108450                      NaN                          0   \n",
       "\n",
       "   ...  bathrooms  bathrooms_text  bedrooms  beds host_is_superhost  \\\n",
       "0  ...        1.0          1 bath       0.0   2.0                 t   \n",
       "1  ...        1.0          1 bath       1.0   1.0                 t   \n",
       "2  ...        1.0          1 bath       2.0   3.0                 f   \n",
       "3  ...        NaN          1 bath       3.0   NaN                 f   \n",
       "4  ...        NaN          1 bath       1.0   NaN                 f   \n",
       "\n",
       "   host_since    license  calculated_host_listings_count  minimum_nights  \\\n",
       "0  2009-11-04  25-156058                               1               2   \n",
       "1  2009-11-07  25-157257                               1               1   \n",
       "2  2009-12-15        NaN                               1               5   \n",
       "3  2009-11-29        NaN                               5             365   \n",
       "4  2010-01-07  21-156705                               1               5   \n",
       "\n",
       "  maximum_nights  \n",
       "0            180  \n",
       "1           1125  \n",
       "2             31  \n",
       "3            365  \n",
       "4             60  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4.3 — Unify listings schema (price, city, month, neighbourhood)\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "month_map = {\n",
    "    'January': '01', 'February': '02', 'March': '03', 'April': '04',\n",
    "    'May': '05', 'June': '06', 'July': '07', 'August': '08',\n",
    "    'September': '09', 'October': '10', 'November': '11', 'December': '12'\n",
    "}\n",
    "\n",
    "city_from_fname = lambda n: 'Vancouver' if n.lower().startswith('van_') else ('Victoria' if n.lower().startswith('vic_') else None)\n",
    "\n",
    "# price parser robust to $ and commas\n",
    "def parse_price(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    if isinstance(val, (int, float)):\n",
    "        return float(val)\n",
    "    s = str(val)\n",
    "    s = re.sub(r'[^0-9.]+', '', s)\n",
    "    try:\n",
    "        return float(s) if s else np.nan\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "# Helper to clean neighbourhood strings\n",
    "def clean_neigh(s: pd.Series) -> pd.Series:\n",
    "    if s is None:\n",
    "        return pd.Series(np.nan, index=range(0))\n",
    "    x = (s.astype(str)\n",
    "           .str.strip()\n",
    "           .str.replace(r'\\s+', ' ', regex=True))\n",
    "    # Remove known placeholder labels (both British/US spelling)\n",
    "    x = x.str.replace(r'(?i)\\bneighbourhood highlights\\b', '', regex=True)\n",
    "    x = x.str.replace(r'(?i)\\bneighborhood highlights\\b', '', regex=True)\n",
    "    # If value becomes empty or 'nan', set NaN\n",
    "    x = x.replace({'': np.nan, 'nan': np.nan, 'None': np.nan})\n",
    "    # Remove leading/trailing punctuation\n",
    "    x = x.str.replace(r'^[^A-Za-z0-9]+|[^A-Za-z0-9]+$', '', regex=True)\n",
    "    # Title-case\n",
    "    x = x.str.title()\n",
    "    return x\n",
    "\n",
    "# Fields try to pass through when present\n",
    "pass_through_fields = [\n",
    "    'estimated_revenue_l365d','estimated_occupancy_l365d','availability_365','availability_30',\n",
    "    'reviews_per_month','number_of_reviews','amenities','property_type','room_type','accommodates',\n",
    "    'bathrooms','bathrooms_text','bedrooms','beds','host_is_superhost','host_since','license',\n",
    "    'calculated_host_listings_count','minimum_nights','maximum_nights'\n",
    "]\n",
    "\n",
    "frames = []\n",
    "for fname, df_l in loaded.items():\n",
    "    city = city_from_fname(fname)\n",
    "    m = re.search(r'(January|February|March|April|May|June|July|August|September|October|November|December)(\\d{4})', fname)\n",
    "    if m:\n",
    "        month_str = f\"{m.group(2)}-{month_map[m.group(1)]}\"\n",
    "    else:\n",
    "        # fallback to last_scraped's month\n",
    "        month_col = schema_map[fname].get('date')\n",
    "        month_str = pd.to_datetime(df_l[month_col], errors='coerce').dt.strftime('%Y-%m').iloc[0] if month_col else np.nan\n",
    "\n",
    "    # choose columns from schema map\n",
    "    price_col = schema_map[fname].get('price')\n",
    "    neigh_col = schema_map[fname].get('neighbourhood')\n",
    "    date_col = schema_map[fname].get('date')\n",
    "    lat_col = schema_map[fname].get('latitude')\n",
    "    lon_col = schema_map[fname].get('longitude')\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out['listing_id'] = df_l['id'] if 'id' in df_l.columns else df_l.index\n",
    "    out['city'] = city\n",
    "    out['month'] = month_str\n",
    "\n",
    "    if neigh_col:\n",
    "        out['neighbourhood'] = clean_neigh(df_l[neigh_col])\n",
    "    else:\n",
    "        out['neighbourhood'] = np.nan\n",
    "\n",
    "    if price_col:\n",
    "        out['price'] = df_l[price_col].apply(parse_price)\n",
    "    else:\n",
    "        out['price'] = np.nan\n",
    "    if date_col:\n",
    "        out['last_scraped'] = pd.to_datetime(df_l[date_col], errors='coerce')\n",
    "\n",
    "    # optional coords using detected exact columns\n",
    "    if lat_col and lat_col in df_l.columns:\n",
    "        out['latitude'] = pd.to_numeric(df_l[lat_col], errors='coerce')\n",
    "    if lon_col and lon_col in df_l.columns:\n",
    "        out['longitude'] = pd.to_numeric(df_l[lon_col], errors='coerce')\n",
    "\n",
    "    # pass-through fields when available\n",
    "    mapping = schema_map[fname]\n",
    "    for key in pass_through_fields:\n",
    "        src = mapping.get(key)\n",
    "        if src and src in df_l.columns:\n",
    "            out[key] = df_l[src]\n",
    "\n",
    "    frames.append(out)\n",
    "\n",
    "listings = pd.concat(frames, ignore_index=True)\n",
    "print('Unified listings shape:', listings.shape)\n",
    "print(listings[['city','month']].value_counts().sort_index())\n",
    "print('\\nColumns:', listings.columns.tolist())\n",
    "listings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437ce880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vancouver area stats (km²): {'count': 24.0, 'mean': 4.678, 'std': 2.162, 'min': 0.639, '25%': 3.262, '50%': 4.467, '75%': 6.29, 'max': 8.581}\n",
      "Victoria area stats (km²): {'count': 29.0, 'mean': 167.061, 'std': 565.565, 'min': 0.24, '25%': 1.579, '50%': 7.334, '75%': 46.992, 'max': 2901.212}\n",
      "Saved cleaned polygons to: cleaned_data\\van_neighbourhoods_clean.geojson and cleaned_data\\vic_neighbourhoods_clean.geojson\n"
     ]
    }
   ],
   "source": [
    "# Step 4.4 — Project polygons to metric CRS and compute area (km²)\n",
    "\n",
    "EPSG_METRIC = 26910  # UTM Zone 10N (BC west)\n",
    "\n",
    "van_prj = van.to_crs(EPSG_METRIC)\n",
    "vic_prj = vic.to_crs(EPSG_METRIC)\n",
    "\n",
    "van_prj['area_km2'] = van_prj.geometry.area / 1e6\n",
    "vic_prj['area_km2'] = vic_prj.geometry.area / 1e6\n",
    "\n",
    "print('Vancouver area stats (km²):', van_prj['area_km2'].describe().round(3).to_dict())\n",
    "print('Victoria area stats (km²):', vic_prj['area_km2'].describe().round(3).to_dict())\n",
    "\n",
    "# persist minimal cleaned polygons\n",
    "cleaned_dir = Path('cleaned_data')\n",
    "cleaned_dir.mkdir(exist_ok=True)\n",
    "van_out = cleaned_dir / 'van_neighbourhoods_clean.geojson'\n",
    "vic_out = cleaned_dir / 'vic_neighbourhoods_clean.geojson'\n",
    "\n",
    "van_prj[['neighbourhood','neighbourhood_group','area_km2','geometry']].to_file(van_out, driver='GeoJSON')\n",
    "vic_prj[['neighbourhood','neighbourhood_group','area_km2','geometry']].to_file(vic_out, driver='GeoJSON')\n",
    "print('Saved cleaned polygons to:', van_out, 'and', vic_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b91c710d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by price [20,10000] -> 17740 -> 14479 rows\n",
      "Cleaned counts by city-month:\n",
      "city       month  \n",
      "Vancouver  2025-04    4312\n",
      "           2025-06    4236\n",
      "Victoria   2025-03    3119\n",
      "           2025-05    2812\n",
      "Name: count, dtype: int64\n",
      "Neighbourhood unique count: 51\n",
      "Saved cleaned listings to: cleaned_data\\listings_unified_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 4.5 — Clean listings (price/date/coords) and persist\n",
    "\n",
    "assert 'listings' in globals(), 'Run Step 4.3 first to build listings'\n",
    "\n",
    "clean = listings.copy()\n",
    "\n",
    "# Coerce date\n",
    "if 'last_scraped' in clean.columns:\n",
    "    clean['last_scraped'] = pd.to_datetime(clean['last_scraped'], errors='coerce')\n",
    "\n",
    "# Filter price outliers (keep reasonable nightly range)\n",
    "low, high = 20, 10000\n",
    "before = len(clean)\n",
    "clean = clean[clean['price'].between(low, high)]\n",
    "print(f'Filtered by price [{low},{high}] -> {before} -> {len(clean)} rows')\n",
    "\n",
    "# Standardize neighbourhood text and remove placeholders\n",
    "clean['neighbourhood'] = (clean['neighbourhood']\n",
    "                          .astype(str)\n",
    "                          .str.strip()\n",
    "                          .str.replace('\\n', ' ', regex=False)\n",
    "                          .str.replace('\\r', ' ', regex=False)\n",
    "                          .str.replace(r'\\s+', ' ', regex=True))\n",
    "clean['neighbourhood'] = clean['neighbourhood'].str.replace(r'(?i)\\bneighbourhood highlights\\b', '', regex=True)\n",
    "clean['neighbourhood'] = clean['neighbourhood'].str.replace(r'(?i)\\bneighborhood highlights\\b', '', regex=True)\n",
    "# Normalize empties to NaN\n",
    "clean['neighbourhood'] = clean['neighbourhood'].replace({'': np.nan, 'nan': np.nan, 'None': np.nan})\n",
    "# Title-case valid values\n",
    "clean.loc[clean['neighbourhood'].notna(), 'neighbourhood'] = clean.loc[clean['neighbourhood'].notna(), 'neighbourhood'].str.title()\n",
    "\n",
    "# Drop rows without neighbourhood or coordinates (for spatial join later)\n",
    "before = len(clean)\n",
    "if {'latitude','longitude'}.issubset(clean.columns):\n",
    "    clean = clean.dropna(subset=['latitude','longitude'])\n",
    "clean = clean.dropna(subset=['neighbourhood'])\n",
    "\n",
    "# Basic stats\n",
    "print('Cleaned counts by city-month:')\n",
    "print(clean[['city','month']].value_counts().sort_index())\n",
    "print('Neighbourhood unique count:', clean['neighbourhood'].nunique())\n",
    "\n",
    "# Persist\n",
    "out_csv = Path('cleaned_data') / 'listings_unified_clean.csv'\n",
    "out_csv.parent.mkdir(exist_ok=True)\n",
    "clean.to_csv(out_csv, index=False)\n",
    "print(f'Saved cleaned listings to: {out_csv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055e93a",
   "metadata": {},
   "source": [
    "## RQ1 — What predicts annual Airbnb revenue? (Listing-level modeling)\n",
    "We build a listing-level dataset, engineer features, derive an annual revenue target from nightly price and an occupancy proxy, and fit models to quantify predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ecd37aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared features for RQ1:\n",
      "  rows: 14479\n",
      "  numeric features: ['price', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'reviews_per_month', 'number_of_reviews', 'availability_365', 'availability_30', 'calculated_host_listings_count', 'minimum_nights', 'maximum_nights', 'amenities_count']\n",
      "  categorical features: ['property_type', 'room_type', 'host_is_superhost', 'license', 'city']\n"
     ]
    }
   ],
   "source": [
    "# RQ1 — Build features and target; define train/test split\n",
    "assert 'clean' in globals(), 'Run Step 4.5 first to build clean listings DataFrame'\n",
    "df0 = pd.read_csv('cleaned_data/listings_unified_clean.csv', low_memory=False)\n",
    "# 2) Minimal feature engineering\n",
    "# amenities_count\n",
    "if 'amenities' in df0.columns:\n",
    "    def amenity_count(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        s = str(x)\n",
    "        try:\n",
    "            v = ast.literal_eval(s)\n",
    "            if isinstance(v, (list, tuple, set)):\n",
    "                return len(v)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # fallback: comma-separated string\n",
    "        return int(len([t for t in s.split(',') if t.strip()]))\n",
    "    df0['amenities_count'] = df0['amenities'].apply(amenity_count)\n",
    "\n",
    "# Normalize host_is_superhost to string category\n",
    "if 'host_is_superhost' in df0.columns:\n",
    "    df0['host_is_superhost'] = (\n",
    "        df0['host_is_superhost']\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .map({'t':'true','true':'true','f':'false','false':'false'})\n",
    "        .fillna('unknown')\n",
    "    )\n",
    "\n",
    "# 3) Create target y (annual revenue)\n",
    "# Preferred: estimated_revenue_l365d\n",
    "if 'estimated_revenue_l365d' in df0.columns and df0['estimated_revenue_l365d'].notna().mean() > 0.3:\n",
    "    y = pd.to_numeric(df0['estimated_revenue_l365d'], errors='coerce')\n",
    "else:\n",
    "    # Fallback: price * occupancy * 365\n",
    "    price = pd.to_numeric(df0.get('price', np.nan), errors='coerce')\n",
    "    occ = None\n",
    "    if 'estimated_occupancy_l365d' in df0.columns:\n",
    "        occ = pd.to_numeric(df0['estimated_occupancy_l365d'], errors='coerce')\n",
    "        if occ.dropna().max() > 1.5:\n",
    "            occ = occ / 100.0  # convert fraction\n",
    "    elif 'availability_365' in df0.columns:\n",
    "        avail = pd.to_numeric(df0['availability_365'], errors='coerce')\n",
    "        occ = 1.0 - (avail / 365.0)\n",
    "    else:\n",
    "        occ = pd.Series(0.5, index=df0.index)  \n",
    "    y = price * occ * 365.0\n",
    "\n",
    "# 4) Select features\n",
    "numeric_features = [\n",
    "    'price','accommodates','bathrooms','bedrooms','beds',\n",
    "    'reviews_per_month','number_of_reviews','availability_365','availability_30',\n",
    "    'calculated_host_listings_count','minimum_nights','maximum_nights','amenities_count'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'property_type','room_type','host_is_superhost','license','city'\n",
    "]\n",
    "\n",
    "# keep only columns that exist\n",
    "numeric_features = [c for c in numeric_features if c in df0.columns]\n",
    "categorical_features = [c for c in categorical_features if c in df0.columns]\n",
    "\n",
    "# Build feature frame\n",
    "features_df = df0[numeric_features + categorical_features].copy()\n",
    "\n",
    "# Ensure categorical fields are strings with no NaN (avoid OHE issues)\n",
    "for c in categorical_features:\n",
    "    features_df[c] = features_df[c].astype('string').fillna('Unknown')\n",
    "\n",
    "# 5) Drop rows with missing target and align\n",
    "mask = y.notna()\n",
    "X = features_df[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# 6) Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "print('Prepared features for RQ1:')\n",
    "print('  rows:', len(X))\n",
    "print('  numeric features:', numeric_features)\n",
    "print('  categorical features:', categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abc5a341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: R2=0.510 | RMSE=18,705.0\n",
      "RandomForest: R2=0.738 | RMSE=13,675.6\n"
     ]
    }
   ],
   "source": [
    "# RQ1 continued: modeling — KNN and RandomForest top-predictor extraction\n",
    "# Warning: This cell contains training large models and may take several minutes to run.\n",
    "# Expect variables from previous cell: features_df, y, X_train, X_test, y_train, y_test, numeric_features, categorical_features\n",
    "\n",
    "assert 'X_train' in globals(), 'Run the feature preparation steps first to define X_train, y_train, etc.'\n",
    "\n",
    "num_feats = [f for f in numeric_features if f in X_train.columns]\n",
    "cat_feats = [f for f in categorical_features if f in X_train.columns]\n",
    "\n",
    "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "# Limit OHE categories by grouping infrequent levels to reduce dimensionality\n",
    "ohe = OneHotEncoder(handle_unknown='infrequent_if_exist', min_frequency=0.01)\n",
    "\n",
    "# Preprocessors\n",
    "prep_knn = ColumnTransformer([\n",
    "    ('num', Pipeline([('imp', SimpleImputer(strategy='median')), ('sc', StandardScaler())]), num_feats),\n",
    "    ('cat', ohe, cat_feats),\n",
    "], remainder='drop')\n",
    "\n",
    "prep_rf = ColumnTransformer([\n",
    "    ('num', SimpleImputer(strategy='median'), num_feats),\n",
    "    ('cat', ohe, cat_feats),\n",
    "], remainder='drop')\n",
    "\n",
    "knn_params = len(num_feats) + len(cat_feats)\n",
    "# Models \n",
    "knn = Pipeline([\n",
    "    ('prep', prep_knn),\n",
    "    ('model', KNeighborsRegressor(n_neighbors=knn_params, weights='distance', metric='minkowski')),\n",
    "])\n",
    "\n",
    "rf = Pipeline([\n",
    "    ('prep', prep_rf),\n",
    "    ('model', RandomForestRegressor(\n",
    "        n_estimators=150,\n",
    "        max_depth=20,\n",
    "        min_samples_leaf=3,\n",
    "        max_features='sqrt',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )),\n",
    "])\n",
    "\n",
    "# Fit\n",
    "knn.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "def rmse_fn(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "metrics = []\n",
    "for name, est in [('KNN', knn), ('RandomForest', rf)]:\n",
    "    pred = est.predict(X_test)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    rmse = rmse_fn(y_test, pred)\n",
    "    metrics.append((name, r2, rmse))\n",
    "    print(f'{name}: R2={r2:.3f} | RMSE={rmse:,.1f}')\n",
    "\n",
    "# Helper to get transformed feature names from a fitted pipeline's 'prep'\n",
    "def get_tnames(fitted_pipe: Pipeline, X_like: pd.DataFrame) -> np.ndarray:\n",
    "    try:\n",
    "        ct = fitted_pipe.named_steps['prep']\n",
    "        names = ct.get_feature_names_out()\n",
    "        # sanity-check: if length doesn't match the transformed shape, adjust\n",
    "        n_out = ct.transform(X_like.iloc[:1]).shape[1]\n",
    "        if len(names) != n_out:\n",
    "            names = np.array([f'f{i}' for i in range(n_out)])\n",
    "        return np.array([str(n) for n in names])\n",
    "    except Exception:\n",
    "        n_out = fitted_pipe.named_steps['prep'].transform(X_like.iloc[:1]).shape[1]\n",
    "        return np.array([f'f{i}' for i in range(n_out)])\n",
    "\n",
    "# Aggregate permutation importances back to base feature names (on a small X_test subset)\n",
    "def aggregate_importance(estimator: Pipeline, X_df: pd.DataFrame, y_ser: pd.Series, n_repeats: int = 3, random_state: int = 42) -> pd.DataFrame:\n",
    "    # Subsample test set for speed\n",
    "    max_eval = 2000\n",
    "    if len(X_df) > max_eval:\n",
    "        X_eval = X_df.sample(n=max_eval, random_state=random_state)\n",
    "        y_eval = y_ser.loc[X_eval.index]\n",
    "    else:\n",
    "        X_eval, y_eval = X_df, y_ser\n",
    "    perm = permutation_importance(estimator, X_eval, y_eval, n_repeats=n_repeats, random_state=random_state, n_jobs=1)\n",
    "    tnames = get_tnames(estimator, X_eval)\n",
    "    n_imp = perm.importances_mean.shape[0]\n",
    "    # Align lengths if needed\n",
    "    if len(tnames) != n_imp:\n",
    "        if len(tnames) > n_imp:\n",
    "            tnames = tnames[:n_imp]\n",
    "        else:\n",
    "            tnames = np.concatenate([tnames, np.array([f'f{i}' for i in range(len(tnames), n_imp)])])\n",
    "    imp = pd.DataFrame({'feature': tnames, 'importance': perm.importances_mean})\n",
    "    imp['feature'] = imp['feature'].str.replace(r'^(num|cat)__', '', regex=True)\n",
    "    # Map one-hot columns back to base categorical names\n",
    "    def to_base(s: str) -> str:\n",
    "        if not cat_feats:\n",
    "            return s\n",
    "        for c in cat_feats:\n",
    "            if s.startswith(c + '_'):\n",
    "                return c\n",
    "        return s\n",
    "    imp['base'] = imp['feature'].map(to_base)\n",
    "    agg = imp.groupby('base', as_index=False)['importance'].sum().sort_values('importance', ascending=False)\n",
    "    return agg\n",
    "\n",
    "# Compute importances\n",
    "try:\n",
    "    imp_knn = aggregate_importance(knn, X_test, y_test)\n",
    "except Exception as e:\n",
    "    print('KNN permutation importance failed:', e)\n",
    "    imp_knn = pd.DataFrame(columns=['base','importance'])\n",
    "\n",
    "try:\n",
    "    imp_rf = aggregate_importance(rf, X_test, y_test)\n",
    "except Exception as e:\n",
    "    print('RF permutation importance failed, falling back to impurity importances:', e)\n",
    "    # Fallback: impurity importances mapped to base names\n",
    "    try:\n",
    "        rf_model = rf.named_steps['model']\n",
    "        tnames = get_tnames(rf, X_test)\n",
    "        fi = getattr(rf_model, 'feature_importances_', None)\n",
    "        if fi is not None and len(fi) == len(tnames):\n",
    "            tmp = pd.DataFrame({'feature': tnames, 'importance': fi})\n",
    "            tmp['feature'] = tmp['feature'].str.replace(r'^(num|cat)__', '', regex=True)\n",
    "            def to_base2(s: str) -> str:\n",
    "                for c in cat_feats:\n",
    "                    if s.startswith(c + '_'):\n",
    "                        return c\n",
    "                return s\n",
    "            tmp['base'] = tmp['feature'].map(to_base2)\n",
    "            imp_rf = tmp.groupby('base', as_index=False)['importance'].sum().sort_values('importance', ascending=False)\n",
    "        else:\n",
    "            imp_rf = pd.DataFrame(columns=['base','importance'])\n",
    "    except Exception as e2:\n",
    "        print('RF impurity importance fallback failed:', e2)\n",
    "        imp_rf = pd.DataFrame(columns=['base','importance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8676bda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 predictors — KNN:\n",
      "                 base  importance\n",
      "13  reviews_per_month    0.166347\n",
      "12      property_type    0.159103\n",
      "11              price    0.138044\n",
      "9      minimum_nights    0.124665\n",
      "1     amenities_count    0.087607\n",
      "\n",
      "Top 5 predictors — RandomForest:\n",
      "                 base  importance\n",
      "11              price    0.426045\n",
      "10  number_of_reviews    0.307581\n",
      "13  reviews_per_month    0.231733\n",
      "9      minimum_nights    0.148630\n",
      "12      property_type    0.113577\n",
      "Saved all ranked predictors to cleaned_data\\top_predictors_rq1.csv\n",
      "\n",
      "Selected final model (by RMSE): RandomForest\n",
      "Saved listing-level predictions to cleaned_data\\predictions_rq1.csv\n",
      "Saved neighbourhood-level predictions to cleaned_data\\predictions_rq1_by_neighbourhood.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\nTop 5 predictors — KNN:')\n",
    "print(imp_knn.head(5))\n",
    "print('\\nTop 5 predictors — RandomForest:')\n",
    "print(imp_rf.head(5))\n",
    "\n",
    "# Report the single top predictor per model\n",
    "top_knn = imp_knn.iloc[0]['base'] if not imp_knn.empty else None\n",
    "top_rf  = imp_rf.iloc[0]['base'] if not imp_rf.empty else None\n",
    "\n",
    "# Save all ranked predictors for both models to a single CSV\n",
    "# Add rank column to all predictors\n",
    "knn_all = imp_knn.copy()\n",
    "knn_all = knn_all.reset_index(drop=True)\n",
    "knn_all.insert(0, 'model', 'KNN')\n",
    "knn_all.insert(1, 'rank', range(1, len(knn_all) + 1))\n",
    "rf_all = imp_rf.copy()\n",
    "rf_all = rf_all.reset_index(drop=True)\n",
    "rf_all.insert(0, 'model', 'RandomForest')\n",
    "rf_all.insert(1, 'rank', range(1, len(rf_all) + 1))\n",
    "combined = pd.concat([\n",
    "    knn_all[['model','rank','base','importance']],\n",
    "    rf_all[['model','rank','base','importance']]\n",
    "], ignore_index=True)\n",
    "combined = combined.rename(columns={'base': 'feature'})\n",
    "out_dir = Path('cleaned_data')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_file = out_dir / 'top_predictors_rq1.csv'\n",
    "combined.to_csv(out_file, index=False)\n",
    "print('Saved all ranked predictors to', out_file)\n",
    "\n",
    "# Choose best by RMSE\n",
    "metrics_sorted = sorted(metrics, key=lambda x: x[2])\n",
    "best_name = metrics_sorted[0][0]\n",
    "best_cv_model = type('Obj', (), {})()\n",
    "best_cv_model.best_estimator_ = knn if best_name == 'KNN' else rf\n",
    "final_model_for_export = rf\n",
    "print(f\"\\nSelected final model (by RMSE): {best_name}\")\n",
    "# After selecting final_model, generate predictions and save both listing-level and neighbourhood-level outputs\n",
    "final_model = globals().get('final_model_for_export', None)\n",
    "if final_model is None:\n",
    "    final_model = best_cv_model.best_estimator_\n",
    "\n",
    "full_preds = pd.Series(final_model.predict(features_df), index=features_df.index, name='predicted_revenue')\n",
    "\n",
    "# Build listing-level output\n",
    "o_cols = ['listing_id','city','month','neighbourhood','price','last_scraped','latitude','longitude',\n",
    "          'estimated_revenue_l365d','estimated_occupancy_l365d','availability_365','availability_30',\n",
    "          'reviews_per_month','number_of_reviews','amenities','property_type','room_type','accommodates',\n",
    "          'bathrooms','bathrooms_text','bedrooms','beds','host_is_superhost','host_since','license',\n",
    "          'calculated_host_listings_count','minimum_nights','maximum_nights']\n",
    "# align o_cols with available in listings\n",
    "o_cols = [c for c in o_cols if c in listings.columns]\n",
    "out = listings[o_cols].copy()\n",
    "out['predicted_revenue'] = full_preds.reindex(out.index)\n",
    "\n",
    "# Save listing-level\n",
    "cleaned_dir.mkdir(parents=True, exist_ok=True)\n",
    "pred_path = cleaned_dir / 'predictions_rq1.csv'\n",
    "out.to_csv(pred_path, index=False)\n",
    "print('Saved listing-level predictions to', pred_path)\n",
    "\n",
    "# Neighbourhood aggregation\n",
    "neigh = neigh_col if 'neigh_col' in globals() else 'neighbourhood'\n",
    "if neigh not in out.columns and 'neighbourhood' in out.columns:\n",
    "    neigh = 'neighbourhood'\n",
    "agg = out.groupby(['city', neigh], as_index=False)['predicted_revenue'].mean().rename(columns={neigh:'neighbourhood'})\n",
    "agg_path = cleaned_dir / 'predictions_rq1_by_neighbourhood.csv'\n",
    "agg.to_csv(agg_path, index=False)\n",
    "print('Saved neighbourhood-level predictions to', agg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48bce84",
   "metadata": {},
   "source": [
    "## RQ2 — Compute ROI from predicted revenue\n",
    "We merge the neighbourhood-level average predicted annual revenue with 2025 benchmark house prices to compute ROI = revenue ÷ price, then rank top neighbourhoods per city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0de69890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved name mapping to cleaned_data\\roi_name_mapping.csv\n",
      "Top neighbourhoods by ROI (predicted_revenue / benchmark_price):\n",
      "         city        neighbourhood  predicted_revenue  benchmark_price  \\\n",
      "33  Vancouver             Downtown       25337.342661         660017.0   \n",
      "10  Vancouver           Strathcona       29079.250746         917658.0   \n",
      "37  Vancouver  Renfrew-Collingwood       25968.415758         900658.0   \n",
      "29   Victoria                Sooke       21406.648195         869400.0   \n",
      "30   Victoria        Victoria West       20652.942473        1020900.0   \n",
      "18   Victoria             Langford       20647.368235        1048000.0   \n",
      "\n",
      "    roi_ratio  \n",
      "33   0.038389  \n",
      "10   0.031689  \n",
      "37   0.028833  \n",
      "29   0.024622  \n",
      "30   0.020230  \n",
      "18   0.019702  \n",
      "Saved: cleaned_data\\roi_vancouver.csv\n",
      "Saved: cleaned_data\\roi_victoria.csv\n",
      "Saved full merge: cleaned_data\\roi_merged_full.csv\n"
     ]
    }
   ],
   "source": [
    "# RQ2 — ROI computation using predictions and 2025 benchmark prices with robust matching (token-based)\n",
    "\n",
    "pred_path = Path('cleaned_data') / 'predictions_rq1_by_neighbourhood.csv'\n",
    "assert pred_path.exists(), 'Run the RQ1 cell to generate neighbourhood-level predictions first.'\n",
    "\n",
    "preds = pd.read_csv(pred_path)\n",
    "# normalize column names\n",
    "preds.columns = [c.strip() for c in preds.columns]\n",
    "if 'neighbourhood' not in preds.columns:\n",
    "    nh_cands = [c for c in preds.columns if 'neigh' in c.lower() or 'hood' in c.lower()]\n",
    "    if nh_cands:\n",
    "        preds.rename(columns={nh_cands[0]: 'neighbourhood'}, inplace=True)\n",
    "if 'city' not in preds.columns:\n",
    "    raise AssertionError('Predictions must contain a city column')\n",
    "if 'predicted_revenue' not in preds.columns:\n",
    "    alt = [c for c in preds.columns if 'pred' in c.lower() and 'rev' in c.lower()] \n",
    "    if alt:\n",
    "        preds.rename(columns={alt[0]: 'predicted_revenue'}, inplace=True)\n",
    "\n",
    "preds['neighbourhood'] = preds['neighbourhood'].astype(str).str.strip()\n",
    "\n",
    "# Helper normalization for name matching\n",
    "def normalize_name(s: str) -> str:\n",
    "    s = (s or '').lower()\n",
    "    s = s.replace('&', ' and ').replace('–', '-')\n",
    "    s = re.sub(r'[^a-z0-9\\- ]+', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    # collapse dashes\n",
    "    s = s.replace('-', ' ')\n",
    "    # drop common noise words\n",
    "    s = re.sub(r'\\b(neighbou?rhood|area|district|community|village|centre|center|city)\\b', ' ', s)\n",
    "    # expand/normalize\n",
    "    s = re.sub(r'\\bmt\\b', 'mount', s)\n",
    "    s = re.sub(r'\\bst\\.?\\b', 'saint', s)\n",
    "    s = re.sub(r'\\b(ve|vw)\\b', '', s)  # REBGV suffixes\n",
    "    # directions\n",
    "    s = re.sub(r'\\b(north|south|east|west|central)\\b', '', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "# token-set similarity (like fuzzywuzzy token_set_ratio)\n",
    "def token_set_similarity(a: str, b: str) -> float:\n",
    "    ta = set(normalize_name(a).split())\n",
    "    tb = set(normalize_name(b).split())\n",
    "    if not ta or not tb:\n",
    "        return 0.0\n",
    "    inter = len(ta & tb)\n",
    "    return (2.0 * inter) / (len(ta) + len(tb))\n",
    "\n",
    "# Parse currency price strings robustly\n",
    "_def_price_re = re.compile(r'[^0-9.]+')\n",
    "\n",
    "def parse_price_str(x):\n",
    "    if pd.isna(x):\n",
    "        return pd.NA\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    s = str(x)\n",
    "    s = _def_price_re.sub('', s)\n",
    "    try:\n",
    "        return float(s) if s else pd.NA\n",
    "    except Exception:\n",
    "        return pd.NA\n",
    "\n",
    "# Helper to load and normalize a price CSV\n",
    "def load_price_csv(path: Path, city_name: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    nh_cands = [c for c in df.columns if any(k in c.lower() for k in ['neigh', 'hood', 'area', 'district', 'community'])]\n",
    "    if not nh_cands:\n",
    "        first_non_num = next((c for c in df.columns if not pd.api.types.is_numeric_dtype(df[c])), None)\n",
    "        if first_non_num is None:\n",
    "            raise KeyError(f'Could not detect neighbourhood column in {path.name}')\n",
    "        nh_col = first_non_num\n",
    "    else:\n",
    "        nh_pref = [c for c in nh_cands if 'neigh' in c.lower() or 'hood' in c.lower()]\n",
    "        nh_col = nh_pref[0] if nh_pref else nh_cands[0]\n",
    "    price_cands = [c for c in df.columns if any(k in c.lower() for k in ['price', 'avg', 'average', 'hpi', 'value', 'benchmark'])]\n",
    "    price_cands = [c for c in price_cands if c != nh_col]\n",
    "    if not price_cands:\n",
    "        num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "        if not num_cols:\n",
    "            raise KeyError(f'Could not detect price column in {path.name}')\n",
    "        price_col = num_cols[0]\n",
    "    else:\n",
    "        pr_pref = [c for c in price_cands if any(k in c.lower() for k in ['price','benchmark'])]\n",
    "        price_col = pr_pref[0] if pr_pref else price_cands[0]\n",
    "\n",
    "    out = df[[nh_col, price_col]].copy()\n",
    "    out.columns = ['neighbourhood', 'benchmark_price']\n",
    "    out['neighbourhood'] = out['neighbourhood'].astype(str).str.strip()\n",
    "    # robust currency parsing\n",
    "    out['benchmark_price'] = out['benchmark_price'].apply(parse_price_str)\n",
    "    out['benchmark_price'] = pd.to_numeric(out['benchmark_price'], errors='coerce')\n",
    "    out = out.dropna(subset=['benchmark_price'])\n",
    "    out['city'] = city_name\n",
    "    return out\n",
    "\n",
    "van_prices = load_price_csv(Path('original_data') / '2025_van_neighbourhoods_price.csv', 'Vancouver')\n",
    "vic_prices = load_price_csv(Path('original_data') / '2025_vic_neighbourhoods_price.csv', 'Victoria')\n",
    "prices = pd.concat([van_prices, vic_prices], ignore_index=True)\n",
    "\n",
    "# Create normalized keys\n",
    "preds['key'] = preds['neighbourhood'].map(normalize_name)\n",
    "prices['key'] = prices['neighbourhood'].map(normalize_name)\n",
    "\n",
    "# Try exact normalized join first\n",
    "merged = preds.merge(prices, on=['city','key'], how='inner', suffixes=('_pred','_price'))\n",
    "\n",
    "# Build token-set fuzzy mapping for unmatched\n",
    "matched_keys = set(merged['key'].unique()) if not merged.empty else set()\n",
    "unmatched = preds[~preds['key'].isin(matched_keys)].copy()\n",
    "rows = []\n",
    "for city_name, grp in unmatched.groupby('city'):\n",
    "    cand_df = prices[prices['city']==city_name]\n",
    "    if cand_df.empty:\n",
    "        continue\n",
    "    cand_keys = cand_df['key'].unique().tolist()\n",
    "    for _, r in grp[['neighbourhood','key']].drop_duplicates().iterrows():\n",
    "        best_score, best_key = 0.0, None\n",
    "        for ck in cand_keys:\n",
    "            # quick filter: at least one shared token\n",
    "            if not set(r['key'].split()) & set(ck.split()):\n",
    "                continue\n",
    "            s = token_set_similarity(r['key'], ck)\n",
    "            if s > best_score:\n",
    "                best_score, best_key = s, ck\n",
    "        if best_key is not None and best_score >= 0.6:\n",
    "            rows.append({'city': city_name, 'key': r['key'], 'key_price': best_key, 'match_score': best_score})\n",
    "\n",
    "map_df = pd.DataFrame(rows)\n",
    "if not map_df.empty:\n",
    "    tmp = preds.merge(map_df[['city','key','key_price']], on=['city','key'], how='left')\n",
    "    tmp['join_key'] = tmp['key_price'].fillna(tmp['key'])\n",
    "    prices2 = prices.rename(columns={'key': 'join_key'})\n",
    "    merged2 = tmp.merge(prices2, on=['city','join_key'], how='inner', suffixes=('_pred','_price'))\n",
    "    if not merged.empty:\n",
    "        existing = set(zip(merged['city'], merged['neighbourhood_pred']))\n",
    "        merged2 = merged2[[ (c not in existing) for c in zip(merged2['city'], merged2['neighbourhood_pred']) ]]\n",
    "        merged = pd.concat([merged, merged2], ignore_index=True)\n",
    "    else:\n",
    "        merged = merged2\n",
    "\n",
    "# Save mapping debug\n",
    "out_dir = Path('cleaned_data')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "if not map_df.empty:\n",
    "    map_df.to_csv(out_dir / 'roi_name_mapping.csv', index=False)\n",
    "    print('Saved name mapping to', out_dir / 'roi_name_mapping.csv')\n",
    "\n",
    "# Compute ROI if we have matches\n",
    "if merged.empty:\n",
    "    # emit some debug samples\n",
    "    print('ROI merge empty. Debug samples:')\n",
    "    print('Pred keys (first 15):', sorted(preds['key'].unique())[:15])\n",
    "    print('Price keys Vancouver (first 15):', sorted(prices[prices['city']=='Vancouver']['key'].unique())[:15])\n",
    "    print('Price keys Victoria (first 15):', sorted(prices[prices['city']=='Victoria']['key'].unique())[:15])\n",
    "else:\n",
    "    merged = merged.rename(columns={'neighbourhood_pred':'neighbourhood'})\n",
    "    merged['predicted_revenue'] = pd.to_numeric(merged['predicted_revenue'], errors='coerce')\n",
    "    merged = merged.dropna(subset=['predicted_revenue','benchmark_price'])\n",
    "    merged['roi_ratio'] = merged['predicted_revenue'] / merged['benchmark_price']\n",
    "\n",
    "    # Top 3 by city (handle <3 gracefully)\n",
    "    top3 = merged.sort_values(['city','roi_ratio'], ascending=[True, False]).groupby('city').head(3)\n",
    "    print('Top neighbourhoods by ROI (predicted_revenue / benchmark_price):')\n",
    "    print(top3[['city','neighbourhood','predicted_revenue','benchmark_price','roi_ratio']])\n",
    "\n",
    "    # Save outputs\n",
    "    top3[top3['city']=='Vancouver'][['city','neighbourhood','predicted_revenue','benchmark_price','roi_ratio']].to_csv(out_dir / 'roi_vancouver.csv', index=False)\n",
    "    top3[top3['city']=='Victoria'][['city','neighbourhood','predicted_revenue','benchmark_price','roi_ratio']].to_csv(out_dir / 'roi_victoria.csv', index=False)\n",
    "    merged[['city','neighbourhood','predicted_revenue','benchmark_price','roi_ratio']].to_csv(out_dir / 'roi_merged_full.csv', index=False)\n",
    "    print('Saved:', out_dir / 'roi_vancouver.csv')\n",
    "    print('Saved:', out_dir / 'roi_victoria.csv')\n",
    "    print('Saved full merge:', out_dir / 'roi_merged_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba36d1b",
   "metadata": {},
   "source": [
    "## RQ3 — Tests (ANOVA first, then post hoc)\n",
    "workflow:\n",
    "1) Two-way ANOVA (City × Period) for average nightly price (listing-level).\n",
    "2) If ANOVA shows significance, run Tukey HSD post hoc on City×Period groups.\n",
    "3) Complementary checks: Welch t-tests (price Pre vs Post within each city), and Chi-square/proportion tests for licence share.\n",
    "Period uses city-specific pre/post lists when available; otherwise month < 2025-05 is Pre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8b1c645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV path: c:\\Users\\alext\\OneDrive\\桌面\\CMPT 353\\Project\\cleaned_data\\listings_unified_clean.csv\n",
      "Assigned Period from month: Pre=[2025-03-01, 2025-04-01], Post=[2025-05-01, 2025-06-01].\n",
      "Rows after cleaning: 14,479; Cities=['Vancouver', 'Victoria']; Periods=['Pre', 'Post']\n",
      "\n",
      "Two-way ANOVA (Price) — City * Period\n",
      "                           sum_sq       df          F        PR(>F)\n",
      "C(_City)             7.690041e+05      1.0  14.665230  1.289343e-04\n",
      "C(_Period)           4.698395e+06      1.0  89.600365  3.356588e-21\n",
      "C(_City):C(_Period)  4.364934e+05      1.0   8.324112  3.918001e-03\n",
      "Residual             7.590290e+08  14475.0        NaN           NaN\n",
      "\n",
      "ANOVA interaction (Price) p=0.003918001138383897; Tukey HSD will run: True\n",
      "\n",
      "Tukey HSD — Price by City * Period\n",
      "           Multiple Comparison of Means - Tukey HSD, FWER=0.05           \n",
      "=========================================================================\n",
      "     group1           group2     meandiff p-adj   lower    upper   reject\n",
      "-------------------------------------------------------------------------\n",
      "Vancouver | Post Vancouver | Pre -45.1886    0.0 -57.9165 -32.4608   True\n",
      "Vancouver | Post Victoria | Post -26.3795    0.0 -40.6911 -12.0679   True\n",
      "Vancouver | Post  Victoria | Pre -49.2199    0.0 -63.1017 -35.3381   True\n",
      " Vancouver | Pre Victoria | Post  18.8091 0.0039    4.548  33.0703   True\n",
      " Vancouver | Pre  Victoria | Pre  -4.0313 0.8772 -17.8611   9.7985  False\n",
      " Victoria | Post  Victoria | Pre -22.8404 0.0007 -38.1403  -7.5405   True\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "Welch t-tests — Price (Post vs Pre) within each City\n",
      "  Vancouver: t=9.031, p=2.077e-19, n_pre=4312, n_post=4236\n",
      "  Victoria: t=3.948, p=7.984e-05, n_pre=3119, n_post=2812\n",
      "\n",
      "Chi-square — License by Period (overall):\n",
      "_lic_bin     0     1\n",
      "_Period             \n",
      "Post      3934  3114\n",
      "Pre       4277  3154\n",
      "chi2=4.385, dof=1, p=0.03626\n",
      "\n",
      "Proportion z-tests — Licensed share (Post vs Pre) per City\n",
      "  Vancouver: z=0.915, p=0.3601, post=0.735, pre=0.726, n_post=4236, n_pre=4312\n",
      "  Victoria: z=-4.462, p=8.124e-06, post=0.000, pre=0.007, n_post=2812, n_pre=3119\n",
      "\n",
      "Saved: rq3_price_summary.csv, rq3_welch_price.csv, rq3_anova_price.csv, rq3_tukey_price.csv, rq3_license_share.csv (if license present), rq3_license_chi2.csv (if ran)\n"
     ]
    }
   ],
   "source": [
    "# RQ3 : read cleaned_data/listings_unified_clean.csv and run tests\n",
    "import numpy as np, pandas as pd, re\n",
    "from pathlib import Path\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from scipy.stats import ttest_ind, chi2_contingency\n",
    "\n",
    "# Locate file robustly\n",
    "root = Path.cwd()\n",
    "csv_path = root / 'cleaned_data' / 'listings_unified_clean.csv'\n",
    "if not csv_path.exists():\n",
    "    # try parent\n",
    "    csv_path = root.parent / 'cleaned_data' / 'listings_unified_clean.csv'\n",
    "print(f'CSV path: {csv_path}')\n",
    "\n",
    "# Output directory for results\n",
    "out_dir = root / 'cleaned_data'\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Helper: find column by candidates (case-insensitive)\n",
    "def find_col(cols, candidates):\n",
    "    low = {c.lower(): c for c in cols}\n",
    "    for name in candidates:\n",
    "        for key, orig in low.items():\n",
    "            if key == name.lower():\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "def find_col_contains_numeric(df, substrings):\n",
    "    for col in df.columns:\n",
    "        if any(s in col.lower() for s in substrings):\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "# Determine core columns\n",
    "city_col = find_col(df.columns, ['city'])\n",
    "month_col = find_col(df.columns, ['month','date','period_month','ym'])\n",
    "period_col = find_col(df.columns, ['Period','period'])\n",
    "price_col = find_col(df.columns, ['price']) or find_col_contains_numeric(df, ['price'])\n",
    "\n",
    "if city_col is None or month_col is None or price_col is None:\n",
    "    raise ValueError(f'Missing required columns. Found city={city_col}, month={month_col}, price={price_col}.')\n",
    "\n",
    "# Normalize months to datetime for ordering and standardized string\n",
    "mtry = pd.to_datetime(df[month_col], errors='coerce')\n",
    "if mtry.isna().all():\n",
    "    mtry = pd.to_datetime(df[month_col].astype(str).str[:7], errors='coerce')\n",
    "\n",
    "# Standardize to first-of-month string for exact matching\n",
    "month_std = mtry.dt.to_period('M').dt.to_timestamp()\n",
    "month_str = month_std.dt.strftime('%Y-%m-01')\n",
    "\n",
    "df['_City'] = df[city_col]\n",
    "df['_MonthDT'] = month_std\n",
    "df['_MonthStr'] = month_str\n",
    "\n",
    "# Determine Period: prefer existing; else derive from known four months; else per-city median split\n",
    "if period_col is not None:\n",
    "    per = df[period_col].astype(str).str.strip().str.title()\n",
    "    df['_Period'] = per.astype('object')\n",
    "else:\n",
    "    uniq = set(df['_MonthStr'].dropna().unique().tolist())\n",
    "    known_four = {'2025-03-01','2025-04-01','2025-05-01','2025-06-01'}\n",
    "    if uniq.issuperset(known_four):\n",
    "        df['_Period'] = pd.Series(pd.NA, index=df.index, dtype='object')\n",
    "        pre_set = {'2025-03-01','2025-04-01'}; post_set = {'2025-05-01','2025-06-01'}\n",
    "        df.loc[df['_MonthStr'].isin(pre_set), '_Period'] = 'Pre'\n",
    "        df.loc[df['_MonthStr'].isin(post_set), '_Period'] = 'Post'\n",
    "        print('Assigned Period from month: Pre=[2025-03-01, 2025-04-01], Post=[2025-05-01, 2025-06-01].')\n",
    "    else:\n",
    "        df['_Period'] = pd.Series(pd.NA, index=df.index, dtype='object')\n",
    "        for c, sub in df.groupby('_City'):\n",
    "            dates = sub['_MonthDT'].dropna().sort_values().unique()\n",
    "            if len(dates) == 0:\n",
    "                continue\n",
    "            cutoff = pd.Series(dates).median()\n",
    "            df.loc[df['_City'].eq(c) & df['_MonthDT'].le(cutoff), '_Period'] = 'Pre'\n",
    "            df.loc[df['_City'].eq(c) & df['_MonthDT'].gt(cutoff), '_Period'] = 'Post'\n",
    "        print('Note: Period column missing and months not exactly the known four; used per-city median month split.')\n",
    "\n",
    "# If no rows assigned to Pre/Post, fallback to median split\n",
    "if df['_Period'].isin(['Pre','Post']).sum() == 0:\n",
    "    df['_Period'] = pd.Series(pd.NA, index=df.index, dtype='object')\n",
    "    for c, sub in df.groupby('_City'):\n",
    "        dates = sub['_MonthDT'].dropna().sort_values().unique()\n",
    "        if len(dates) == 0:\n",
    "            continue\n",
    "        cutoff = pd.Series(dates).median()\n",
    "        df.loc[df['_City'].eq(c) & df['_MonthDT'].le(cutoff), '_Period'] = 'Pre'\n",
    "        df.loc[df['_City'].eq(c) & df['_MonthDT'].gt(cutoff), '_Period'] = 'Post'\n",
    "    print('Fallback applied: per-city median month split used because explicit mapping produced zero rows.')\n",
    "\n",
    "# Keep only rows with defined Period and numeric price\n",
    "work = df.loc[df['_Period'].isin(['Pre','Post']) & pd.notna(df[price_col])].copy()\n",
    "work.rename(columns={price_col: '_Price'}, inplace=True)\n",
    "\n",
    "print(f'Rows after cleaning: {len(work):,}; Cities={sorted(work._City.unique())}; Periods={work._Period.unique().tolist()}')\n",
    "\n",
    "# Two-way ANOVA (Price) — City × Period\n",
    "anova_path = out_dir / 'rq3_anova_price.csv'\n",
    "tukey_path = out_dir / 'rq3_tukey_price.csv'\n",
    "price_summary_path = out_dir / 'rq3_price_summary.csv'\n",
    "welch_summary_path = out_dir / 'rq3_welch_price.csv'\n",
    "license_share_path = out_dir / 'rq3_license_share.csv'\n",
    "license_chi2_path = out_dir / 'rq3_license_chi2.csv'\n",
    "\n",
    "if work._City.nunique() >= 2 and work._Period.nunique() >= 2:\n",
    "    mdl = smf.ols('_Price ~ C(_City) * C(_Period)', data=work).fit()\n",
    "    aov = sm.stats.anova_lm(mdl, typ=2)\n",
    "    print('\\nTwo-way ANOVA (Price) — City * Period')\n",
    "    print(aov)\n",
    "    aov.to_csv(anova_path, index=True)\n",
    "    p_int = aov.loc['C(_City):C(_Period)', 'PR(>F)'] if 'C(_City):C(_Period)' in aov.index else np.nan\n",
    "    do_tukey = (pd.notna(p_int) and p_int < 0.05)\n",
    "    print(f\"\\nANOVA interaction (Price) p={p_int if pd.notna(p_int) else 'NA'}; Tukey HSD will run: {bool(do_tukey)}\\n\")\n",
    "    if do_tukey:\n",
    "        grp = work['_City'] + ' | ' + work['_Period']\n",
    "        tuk = pairwise_tukeyhsd(work['_Price'].values, grp.values, alpha=0.05)\n",
    "        print('Tukey HSD — Price by City * Period')\n",
    "        print(tuk.summary())\n",
    "        # Save Tukey as CSV — reconstruct pair labels in default order (i<j)\n",
    "        groups = list(tuk.groupsunique)\n",
    "        pairs = []\n",
    "        for i in range(len(groups)-1):\n",
    "            for j in range(i+1, len(groups)):\n",
    "                pairs.append((groups[i], groups[j]))\n",
    "        tuk_df = pd.DataFrame({\n",
    "            'group1': [p[0] for p in pairs],\n",
    "            'group2': [p[1] for p in pairs],\n",
    "            'meandiff': tuk.meandiffs,\n",
    "            'p_adj': tuk.pvalues,\n",
    "            'lower': tuk.confint[:,0],\n",
    "            'upper': tuk.confint[:,1],\n",
    "            'reject': tuk.reject\n",
    "        })\n",
    "        tuk_df.to_csv(tukey_path, index=False)\n",
    "else:\n",
    "    print('\\nANOVA skipped: need at least 2 cities and 2 periods with data.')\n",
    "\n",
    "# Price summaries and Welch t-tests — Post vs Pre within each City\n",
    "print('\\nWelch t-tests — Price (Post vs Pre) within each City')\n",
    "# Summary means/stats for plotting\n",
    "price_summary = (\n",
    "    work.groupby(['_City','_Period'])['_Price']\n",
    "        .agg(mean='mean', std='std', n='count')\n",
    "        .reset_index()\n",
    "        .rename(columns={'_City':'City','_Period':'Period','mean':'mean_price','std':'std_price','n':'n'})\n",
    ")\n",
    "price_summary.to_csv(price_summary_path, index=False)\n",
    "\n",
    "welch_rows = []\n",
    "for c in sorted(work['_City'].unique()):\n",
    "    pre_vals = work.loc[(work['_City']==c) & (work['_Period']=='Pre'), '_Price']\n",
    "    post_vals = work.loc[(work['_City']==c) & (work['_Period']=='Post'), '_Price']\n",
    "    m_pre, m_post = pre_vals.mean(), post_vals.mean()\n",
    "    if pre_vals.count() >= 2 and post_vals.count() >= 2:\n",
    "        t_stat, p_val = ttest_ind(post_vals, pre_vals, equal_var=False, nan_policy='omit')\n",
    "        print(f\"  {c}: t={t_stat:.3f}, p={p_val:.4g}, n_pre={pre_vals.count()}, n_post={post_vals.count()}\")\n",
    "        welch_rows.append({'City': c, 't': t_stat, 'p': p_val, 'n_pre': int(pre_vals.count()), 'n_post': int(post_vals.count()), 'mean_pre': m_pre, 'mean_post': m_post, 'diff_post_minus_pre': m_post - m_pre})\n",
    "    else:\n",
    "        print(f\"  {c}: skipped (need ≥2 obs in both Pre and Post)\")\n",
    "welch_df = pd.DataFrame(welch_rows)\n",
    "welch_df.to_csv(welch_summary_path, index=False)\n",
    "\n",
    "# License tests — Chi-square and per-city proportion z-tests\n",
    "lic_col = find_col(df.columns, ['Licensed','licensed','is_licensed','license','has_license'])\n",
    "if lic_col is None:\n",
    "    print('\\nLicense tests skipped: no license indicator column found in CSV.')\n",
    "else:\n",
    "    pl = df.copy()\n",
    "    pl['_City'] = pl['_City']\n",
    "    pl['_Period'] = df['_Period']\n",
    "\n",
    "    # Map license to binary using regex-friendly patterns\n",
    "    def license_to_bin(v):\n",
    "        if pd.isna(v):\n",
    "            return 0\n",
    "        s = str(v).strip()\n",
    "        if not s:\n",
    "            return 0\n",
    "        s_low = s.lower()\n",
    "        # obvious non-license notes\n",
    "        if 'long term' in s_low or '90+' in s_low:\n",
    "            return 0\n",
    "        if s_low in {'am','pm'}:\n",
    "            return 0\n",
    "        # match license-like IDs: '25-156058' or 6+ digits anywhere in the string\n",
    "        if re.search(r'\\b\\d{2}-\\d{5,}\\b', s):\n",
    "            return 1\n",
    "        if re.search(r'\\b\\d{6,}\\b', s):\n",
    "            return 1\n",
    "        # common affirmative tokens\n",
    "        if s_low in {'yes','y','true','t','licensed'}:\n",
    "            return 1\n",
    "        # explicit negatives\n",
    "        if s_low in {'no','n','false','f','unlicensed'}:\n",
    "            return 0\n",
    "        return 0\n",
    "\n",
    "    pl['_lic_bin'] = pl[lic_col].map(license_to_bin).astype(int)\n",
    "    pl = pl.dropna(subset=['_City','_Period'])\n",
    "    pl = pl[pl['_Period'].isin(['Pre','Post'])]\n",
    "\n",
    "    if len(pl)==0 or pl['_lic_bin'].nunique()<2 or pl['_Period'].nunique()<2:\n",
    "        print('\\nChi-square skipped: not enough variation or missing Pre/Post after cleaning.')\n",
    "    else:\n",
    "        tab = pd.crosstab(pl['_Period'], pl['_lic_bin'])\n",
    "        if tab.shape[0] >= 2 and tab.shape[1] >= 2:\n",
    "            chi2, p, dof, exp = chi2_contingency(tab.values)\n",
    "            print('\\nChi-square — License by Period (overall):')\n",
    "            print(tab)\n",
    "            print(f\"chi2={chi2:.3f}, dof={dof}, p={p:.4g}\")\n",
    "            pd.DataFrame([{'chi2': chi2, 'dof': dof, 'p': p}]).to_csv(license_chi2_path, index=False)\n",
    "        else:\n",
    "            print('\\nChi-square skipped: contingency not at least 2x2 after cleaning.')\n",
    "\n",
    "        # Per-city proportion z-tests — and save share summary for plots\n",
    "        share_rows = []\n",
    "        print('\\nProportion z-tests — Licensed share (Post vs Pre) per City')\n",
    "        for c in sorted(pl['_City'].unique()):\n",
    "            sub = pl[pl['_City'].eq(c)]\n",
    "            g = sub.groupby('_Period')['_lic_bin'].agg(['sum','count'])\n",
    "            if {'Pre','Post'}.issubset(g.index):\n",
    "                count = np.array([g.loc['Post','sum'], g.loc['Pre','sum']], dtype=float)\n",
    "                nobs = np.array([g.loc['Post','count'], g.loc['Pre','count']], dtype=float)\n",
    "                if nobs.min() > 0:\n",
    "                    z, pz = proportions_ztest(count, nobs)\n",
    "                    share_post = count[0]/nobs[0]; share_pre = count[1]/nobs[1]\n",
    "                    print(f\"  {c}: z={z:.3f}, p={pz:.4g}, post={share_post:.3f}, pre={share_pre:.3f}, n_post={nobs[0]:.0f}, n_pre={nobs[1]:.0f}\")\n",
    "                    share_rows.append({'City': c, 'Period': 'Pre',  'licensed_share': share_pre,  'licensed_n': int(count[1]), 'n': int(nobs[1])})\n",
    "                    share_rows.append({'City': c, 'Period': 'Post', 'licensed_share': share_post, 'licensed_n': int(count[0]), 'n': int(nobs[0])})\n",
    "                else:\n",
    "                    print(f\"  {c}: skipped (zero observations in Pre or Post)\")\n",
    "            else:\n",
    "                print(f\"  {c}: skipped (missing Pre or Post)\")\n",
    "        share_df = pd.DataFrame(share_rows)\n",
    "        share_df.to_csv(license_share_path, index=False)\n",
    "\n",
    "print(f\"\\nSaved: {price_summary_path.name}, {welch_summary_path.name}, {anova_path.name}, {tukey_path.name if (out_dir/tukey_path.name).exists() else 'tukey (none)'}, {license_share_path.name} (if license present), {license_chi2_path.name} (if ran)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
